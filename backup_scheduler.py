# backup_scheduler.py - Backup vers GitHub Releases - VERSION CORRIG√âE
import os
import sqlite3
import schedule
import time
import logging
import json
import zipfile
import threading
import requests
from datetime import datetime, timedelta
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GitHubBackupManager:
    """Gestionnaire de sauvegardes automatiques vers GitHub Releases - VERSION CORRIG√âE"""
    
    def __init__(self):
        self.config = {
            'db_path': os.environ.get('DB_PATH', '/opt/render/project/data/erp_production_dg.db'),
            'backup_local_dir': os.environ.get('BACKUP_LOCAL_DIR', '/opt/render/project/data/backups'),
            
            # Configuration GitHub
            'github_enabled': os.environ.get('GITHUB_BACKUP_ENABLED', 'true').lower() == 'true',
            'github_token': os.environ.get('GITHUB_TOKEN'),
            'github_repo': os.environ.get('GITHUB_REPO', 'Estimation79/gestion-projets-dg'),
            'github_api_url': 'https://api.github.com',
            
            'keep_local_backups': int(os.environ.get('KEEP_LOCAL_BACKUPS', '5')),
            'keep_github_releases': int(os.environ.get('KEEP_GITHUB_RELEASES', '10')),
            'max_backup_size_mb': int(os.environ.get('MAX_BACKUP_SIZE_MB', '100')),
            
            # NOUVELLES VARIABLES DEBUG CORRIG√âES
            'backup_schedule_minutes': int(os.environ.get('BACKUP_SCHEDULE_MINUTES', '120')),
            'immediate_backup_test': os.environ.get('IMMEDIATE_BACKUP_TEST', 'false').lower() == 'true',
            'force_backup_on_start': os.environ.get('FORCE_BACKUP_ON_START', 'false').lower() == 'true',
            'debug_github_backup': os.environ.get('DEBUG_GITHUB_BACKUP', 'false').lower() == 'true'
        }
        
        Path(self.config['backup_local_dir']).mkdir(parents=True, exist_ok=True)
        self._validate_github_config()
        
        # NOUVEAU : Logging debug activ√©
        if self.config['debug_github_backup']:
            logging.getLogger().setLevel(logging.DEBUG)
            logger.debug("üîç Mode DEBUG backup activ√©")
    
    def _validate_github_config(self):
        """Valide la configuration GitHub - VERSION AM√âLIOR√âE"""
        if not self.config['github_enabled']:
            logger.info("üìä Backup GitHub d√©sactiv√©")
            return
        
        if not self.config['github_token']:
            logger.error("‚ùå GITHUB_TOKEN manquant")
            self.config['github_enabled'] = False
            return
        
        # AM√âLIORATION : Test de connexion plus robuste
        try:
            headers = {'Authorization': f'token {self.config["github_token"]}'}
            
            # Test 1 : V√©rifier l'acc√®s au repo
            repo_response = requests.get(
                f"{self.config['github_api_url']}/repos/{self.config['github_repo']}", 
                headers=headers, timeout=10
            )
            
            if repo_response.status_code == 200:
                repo_info = repo_response.json()
                logger.info(f"‚úÖ Repo GitHub accessible: {repo_info['full_name']}")
                
                # Test 2 : V√©rifier les permissions releases
                releases_response = requests.get(
                    f"{self.config['github_api_url']}/repos/{self.config['github_repo']}/releases",
                    headers=headers, timeout=10
                )
                
                if releases_response.status_code == 200:
                    logger.info("‚úÖ Permissions releases OK")
                    return True
                else:
                    logger.error(f"‚ùå Pas d'acc√®s aux releases: {releases_response.status_code}")
                    
            else:
                logger.error(f"‚ùå Erreur acc√®s repo: {repo_response.status_code}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur connexion GitHub: {e}")
        
        self.config['github_enabled'] = False
        return False
    
    def create_backup(self):
        """Cr√©e une sauvegarde de la base de donn√©es - AVEC DEBUG"""
        try:
            logger.info("üöÄ D√âBUT cr√©ation backup")
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_name = f"erp_dg_backup_{timestamp}"
            
            if not os.path.exists(self.config['db_path']):
                logger.error(f"‚ùå Base de donn√©es non trouv√©e: {self.config['db_path']}")
                return None
            
            # AM√âLIORATION : V√©rification taille DB
            db_size_mb = os.path.getsize(self.config['db_path']) / (1024*1024)
            logger.info(f"üìä Taille DB √† sauvegarder: {db_size_mb:.2f} MB")
            
            # Sauvegarde SQLite
            backup_db_path = os.path.join(self.config['backup_local_dir'], f"{backup_name}.db")
            logger.debug(f"üìÅ Chemin backup DB: {backup_db_path}")
            
            source_conn = sqlite3.connect(self.config['db_path'])
            backup_conn = sqlite3.connect(backup_db_path)
            
            with backup_conn:
                source_conn.backup(backup_conn)
            
            source_conn.close()
            backup_conn.close()
            logger.info("‚úÖ Backup SQLite termin√©")
            
            # M√©tadonn√©es d√©taill√©es
            stats = self._get_database_stats(backup_db_path)
            metadata = {
                'backup_time': datetime.now().isoformat(),
                'backup_time_readable': datetime.now().strftime('%d/%m/%Y √† %H:%M:%S'),
                'backup_size_mb': round(os.path.getsize(backup_db_path) / (1024*1024), 2),
                'company': 'Desmarais & Gagn√© Inc.',
                'database_stats': stats,
                'github_repo': self.config['github_repo'],
                'render_info': {
                    'service_id': os.environ.get('RENDER_SERVICE_ID', 'unknown'),
                    'git_commit': os.environ.get('RENDER_GIT_COMMIT', 'unknown')[:8],
                    'deploy_id': os.environ.get('RENDER_DEPLOY_ID', 'unknown')
                }
            }
            
            metadata_path = os.path.join(self.config['backup_local_dir'], f"{backup_name}.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            # Compression optimis√©e
            zip_path = os.path.join(self.config['backup_local_dir'], f"{backup_name}.zip")
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zipf:
                zipf.write(backup_db_path, f"{backup_name}.db")
                zipf.write(metadata_path, f"{backup_name}_info.json")
                
                # Ajouter un README dans le ZIP
                readme_content = self._create_backup_readme(metadata)
                zipf.writestr("README_BACKUP.md", readme_content)
            
            # Nettoyage fichiers temporaires
            os.remove(backup_db_path)
            os.remove(metadata_path)
            
            final_size_mb = round(os.path.getsize(zip_path) / (1024*1024), 2)
            logger.info(f"‚úÖ Backup ZIP cr√©√©: {final_size_mb} MB")
            logger.info(f"üìÅ Chemin final: {zip_path}")
            
            return zip_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation sauvegarde: {e}")
            import traceback
            logger.error(f"üìã Traceback: {traceback.format_exc()}")
            return None
    
    def _get_database_stats(self, db_path):
        """R√©cup√®re les statistiques compl√®tes de la base"""
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            stats = {}
            tables = ['projects', 'companies', 'employees', 'formulaires', 'work_centers', 
                     'operations', 'materials', 'time_entries', 'contacts', 'interactions']
            
            for table in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    stats[table] = cursor.fetchone()[0]
                except:
                    stats[table] = 0
            
            # Statistiques additionnelles
            try:
                cursor.execute("SELECT COUNT(*) FROM projects WHERE statut = 'EN COURS'")
                stats['projects_active'] = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM projects WHERE statut = 'TERMIN√â'")
                stats['projects_completed'] = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM time_entries WHERE DATE(start_time) = DATE('now')")
                stats['timetracker_today'] = cursor.fetchone()[0]
                
            except:
                pass
            
            stats['total_records'] = sum(v for k, v in stats.items() if not k.startswith('projects_') and k != 'timetracker_today')
            conn.close()
            
            logger.debug(f"üìä Stats DB: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"Erreur stats DB: {e}")
            return {}
    
    def _create_backup_readme(self, metadata):
        """Cr√©e un README pour le backup"""
        stats = metadata.get('database_stats', {})
        return f"""# üè≠ ERP Production DG Inc. - Sauvegarde

## üìã Informations G√©n√©rales
- **Entreprise:** {metadata['company']}
- **Date de sauvegarde:** {metadata['backup_time_readable']}
- **Taille:** {metadata['backup_size_mb']} MB
- **Repository:** {metadata['github_repo']}

## üìä Contenu de la Base de Donn√©es
- **Projets:** {stats.get('projects', 0)} (dont {stats.get('projects_active', 0)} actifs)
- **Entreprises/Clients:** {stats.get('companies', 0)}
- **Employ√©s:** {stats.get('employees', 0)}
- **Formulaires:** {stats.get('formulaires', 0)}
- **Postes de Travail:** {stats.get('work_centers', 0)}
- **Op√©rations:** {stats.get('operations', 0)}
- **Mat√©riaux:** {stats.get('materials', 0)}
- **Pointages:** {stats.get('time_entries', 0)}
- **Contacts:** {stats.get('contacts', 0)}

**Total des enregistrements:** {stats.get('total_records', 0):,}

## üîß Informations Techniques
- **Service Render:** {metadata.get('render_info', {}).get('service_id', 'N/A')}
- **Commit Git:** {metadata.get('render_info', {}).get('git_commit', 'N/A')}
- **Deploy ID:** {metadata.get('render_info', {}).get('deploy_id', 'N/A')}

## üìÅ Fichiers inclus
- `erp_dg_backup_YYYYMMDD_HHMMSS.db` - Base de donn√©es SQLite compl√®te
- `erp_dg_backup_YYYYMMDD_HHMMSS_info.json` - M√©tadonn√©es d√©taill√©es
- `README_BACKUP.md` - Cette documentation

## üîÑ Restauration
Pour restaurer cette sauvegarde :
1. Extraire le fichier ZIP
2. Remplacer le fichier `erp_production_dg.db` par le fichier de backup
3. Red√©marrer l'application ERP

---
ü§ñ Sauvegarde automatique g√©n√©r√©e par le syst√®me ERP DG Inc.
"""
    
    def upload_to_github(self, backup_path):
        """Upload la sauvegarde vers GitHub Releases - VERSION CORRIG√âE"""
        if not self.config['github_enabled']:
            logger.warning("üìä Upload GitHub d√©sactiv√©")
            return False
        
        try:
            logger.info("üöÄ D√âBUT upload GitHub")
            
            file_size_mb = os.path.getsize(backup_path) / (1024 * 1024)
            logger.info(f"üìä Taille fichier √† uploader: {file_size_mb:.2f} MB")
            
            if file_size_mb > self.config['max_backup_size_mb']:
                logger.error(f"üìÅ Fichier trop volumineux ({file_size_mb:.1f}MB > {self.config['max_backup_size_mb']}MB)")
                return False
            
            backup_filename = os.path.basename(backup_path)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # 1. Cr√©er une release
            release_data = {
                'tag_name': f'backup-{timestamp}',
                'name': f'üè≠ ERP Backup - {datetime.now().strftime("%d/%m/%Y %H:%M")}',
                'body': self._create_release_description(backup_path),
                'draft': False,
                'prerelease': True
            }
            
            headers = {
                'Authorization': f'token {self.config["github_token"]}',
                'Content-Type': 'application/json'
            }
            
            logger.info("üì¶ Cr√©ation de la release GitHub...")
            logger.debug(f"üîß URL: {self.config['github_api_url']}/repos/{self.config['github_repo']}/releases")
            logger.debug(f"üîß Headers: {headers}")
            logger.debug(f"üîß Data: {release_data}")
            
            release_response = requests.post(
                f"{self.config['github_api_url']}/repos/{self.config['github_repo']}/releases",
                headers=headers,
                json=release_data,
                timeout=30
            )
            
            logger.info(f"üìã Response status: {release_response.status_code}")
            
            if release_response.status_code != 201:
                logger.error(f"‚ùå Erreur cr√©ation release: {release_response.status_code}")
                logger.error(f"üìã Response body: {release_response.text}")
                return False
            
            release_info = release_response.json()
            upload_url = release_info['upload_url'].replace('{?name,label}', '')
            logger.info(f"‚úÖ Release cr√©√©e: {release_info['html_url']}")
            
            # 2. Upload du fichier
            logger.info("üì§ Upload du fichier backup...")
            
            with open(backup_path, 'rb') as f:
                upload_headers = {
                    'Authorization': f'token {self.config["github_token"]}',
                    'Content-Type': 'application/zip'
                }
                
                logger.debug(f"üîß Upload URL: {upload_url}?name={backup_filename}")
                
                upload_response = requests.post(
                    f"{upload_url}?name={backup_filename}&label=ERP Database Backup",
                    headers=upload_headers,
                    data=f,
                    timeout=120
                )
            
            logger.info(f"üìã Upload status: {upload_response.status_code}")
            
            if upload_response.status_code == 201:
                download_url = upload_response.json()['browser_download_url']
                logger.info(f"‚úÖ Backup upload√© vers GitHub: {download_url}")
                
                # Nettoyage des anciennes releases
                self._cleanup_old_github_releases()
                
                return True
            else:
                logger.error(f"‚ùå Erreur upload: {upload_response.status_code}")
                logger.error(f"üìã Response body: {upload_response.text}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur upload GitHub: {e}")
            import traceback
            logger.error(f"üìã Traceback: {traceback.format_exc()}")
            return False
    
    def _create_release_description(self, backup_path):
        """Cr√©e la description de la release"""
        try:
            # Charger les m√©tadonn√©es
            with zipfile.ZipFile(backup_path, 'r') as zipf:
                for file in zipf.namelist():
                    if file.endswith('_info.json'):
                        with zipf.open(file) as json_file:
                            metadata = json.load(json_file)
                            break
                else:
                    metadata = {}
            
            stats = metadata.get('database_stats', {})
            file_size_mb = round(os.path.getsize(backup_path) / (1024*1024), 2)
            
            return f"""# üè≠ Sauvegarde Automatique ERP DG Inc.

## üìã Informations
- **üìÖ Date:** {metadata.get('backup_time_readable', 'N/A')}
- **üìÅ Taille:** {file_size_mb} MB
- **üè¢ Entreprise:** Desmarais & Gagn√© Inc.

## üìä Contenu de la Base
| Module | Enregistrements |
|--------|-----------------|
| Projets | {stats.get('projects', 0):,} |
| Entreprises | {stats.get('companies', 0):,} |
| Employ√©s | {stats.get('employees', 0):,} |
| Formulaires | {stats.get('formulaires', 0):,} |
| Postes Travail | {stats.get('work_centers', 0):,} |
| Pointages | {stats.get('time_entries', 0):,} |
| **TOTAL** | **{stats.get('total_records', 0):,}** |

## üì¶ Utilisation
1. T√©l√©charger le fichier ZIP
2. Extraire le contenu
3. Utiliser le fichier `.db` pour restaurer l'ERP

---
ü§ñ Sauvegarde automatique g√©n√©r√©e par le syst√®me corrig√©
"""
            
        except Exception as e:
            logger.error(f"Erreur cr√©ation description: {e}")
            return f"""# üè≠ Sauvegarde ERP DG Inc.

**Date:** {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}

Sauvegarde automatique de la base de donn√©es ERP Production.
"""
    
    def _cleanup_old_github_releases(self):
        """Supprime les anciennes releases de backup"""
        try:
            headers = {'Authorization': f'token {self.config["github_token"]}'}
            
            response = requests.get(
                f"{self.config['github_api_url']}/repos/{self.config['github_repo']}/releases",
                headers=headers,
                timeout=30
            )
            
            if response.status_code != 200:
                return
            
            releases = response.json()
            backup_releases = [r for r in releases if r['tag_name'].startswith('backup-')]
            backup_releases.sort(key=lambda x: x['created_at'], reverse=True)
            
            releases_to_delete = backup_releases[self.config['keep_github_releases']:]
            
            for release in releases_to_delete:
                delete_response = requests.delete(
                    f"{self.config['github_api_url']}/repos/{self.config['github_repo']}/releases/{release['id']}",
                    headers=headers,
                    timeout=30
                )
                
                if delete_response.status_code == 204:
                    logger.info(f"üóëÔ∏è Ancienne release supprim√©e: {release['tag_name']}")
                    
            if releases_to_delete:
                logger.info(f"üßπ {len(releases_to_delete)} ancienne(s) release(s) supprim√©e(s)")
                
        except Exception as e:
            logger.error(f"Erreur nettoyage GitHub: {e}")
    
    def cleanup_old_backups(self):
        """Nettoie les anciennes sauvegardes locales"""
        try:
            backup_files = []
            for file in Path(self.config['backup_local_dir']).iterdir():
                if file.is_file() and file.suffix == '.zip':
                    backup_files.append((file, file.stat().st_mtime))
            
            backup_files.sort(key=lambda x: x[1], reverse=True)
            files_to_delete = backup_files[self.config['keep_local_backups']:]
            
            for file_path, _ in files_to_delete:
                file_path.unlink()
                logger.debug(f"üóëÔ∏è Fichier local supprim√©: {file_path}")
            
            if files_to_delete:
                logger.info(f"üßπ {len(files_to_delete)} sauvegarde(s) locale(s) supprim√©e(s)")
                
        except Exception as e:
            logger.error(f"Erreur nettoyage local: {e}")
    
    def run_backup_cycle(self):
        """Cycle complet de sauvegarde - VERSION CORRIG√âE"""
        logger.info("üöÄ ===== D√âBUT CYCLE SAUVEGARDE GITHUB =====")
        
        try:
            # AM√âLIORATION : Test de validit√© avant backup
            if not self.config['github_enabled']:
                logger.warning("‚ö†Ô∏è GitHub backup d√©sactiv√© - cycle annul√©")
                return False
            
            # Cr√©er backup
            backup_path = self.create_backup()
            
            if backup_path:
                logger.info(f"‚úÖ Backup cr√©√©: {backup_path}")
                
                # Upload vers GitHub
                github_success = self.upload_to_github(backup_path)
                
                # Nettoyage
                self.cleanup_old_backups()
                
                if github_success:
                    logger.info("‚úÖ ===== CYCLE TERMIN√â AVEC SUCC√àS =====")
                    return True
                else:
                    logger.warning("‚ö†Ô∏è ===== CYCLE TERMIN√â AVEC AVERTISSEMENT =====")
                    logger.warning("   Backup local OK, GitHub KO")
                    return False
            else:
                logger.error("‚ùå ===== CYCLE √âCHOU√â =====")
                logger.error("   Impossible de cr√©er la sauvegarde")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå ===== ERREUR CYCLE =====")
            logger.error(f"   Exception: {e}")
            import traceback
            logger.error(f"   Traceback: {traceback.format_exc()}")
            return False

# Scheduler CORRIG√â
def start_backup_scheduler():
    """Lance le scheduler GitHub backup - VERSION CORRIG√âE"""
    try:
        backup_manager = GitHubBackupManager()
        
        # CORRECTION 1 : Utiliser les variables d'environnement
        schedule_minutes = backup_manager.config['backup_schedule_minutes']
        
        logger.info(f"‚è∞ Configuration scheduler:")
        logger.info(f"   üîÑ Fr√©quence: {schedule_minutes} minutes")
        logger.info(f"   üì¶ GitHub enabled: {backup_manager.config['github_enabled']}")
        logger.info(f"   üß™ Test imm√©diat: {backup_manager.config['immediate_backup_test']}")
        logger.info(f"   üöÄ Force startup: {backup_manager.config['force_backup_on_start']}")
        
        # CORRECTION 2 : Programmer selon la variable d'environnement
        if schedule_minutes < 60:
            schedule.every(schedule_minutes).minutes.do(backup_manager.run_backup_cycle)
            logger.info(f"   üìÖ Programm√©: Toutes les {schedule_minutes} minutes")
        else:
            schedule.every(schedule_minutes // 60).hours.do(backup_manager.run_backup_cycle)
            logger.info(f"   üìÖ Programm√©: Toutes les {schedule_minutes // 60} heures")
        
        # CORRECTION 3 : Test imm√©diat si demand√©
        if backup_manager.config['immediate_backup_test'] or backup_manager.config['force_backup_on_start']:
            logger.info("üß™ Ex√©cution test imm√©diat...")
            backup_manager.run_backup_cycle()
        
        # CORRECTION 4 : Boucle scheduler am√©lior√©e
        logger.info("üéØ Scheduler GitHub backup d√©marr√© !")
        
        while True:
            schedule.run_pending()
            time.sleep(30)  # V√©rifier toutes les 30 secondes
            
    except Exception as e:
        logger.error(f"‚ùå Erreur scheduler GitHub: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

# CORRECTION 5 : Configuration de d√©marrage am√©lior√©e
def setup_github_backup_info():
    """Affiche les informations de configuration au d√©marrage - VERSION CORRIG√âE"""
    logger.info("üöÄ GITHUB RELEASES BACKUP SYSTEM - VERSION CORRIG√âE")
    logger.info("=" * 60)
    
    # Variables de configuration
    config_vars = {
        'GITHUB_BACKUP_ENABLED': os.environ.get('GITHUB_BACKUP_ENABLED', 'NON D√âFINI'),
        'GITHUB_TOKEN': '‚úÖ Configur√©' if os.environ.get('GITHUB_TOKEN') else '‚ùå MANQUANT',
        'GITHUB_REPO': os.environ.get('GITHUB_REPO', 'NON D√âFINI'),
        'KEEP_GITHUB_RELEASES': os.environ.get('KEEP_GITHUB_RELEASES', '10'),
        'BACKUP_SCHEDULE_MINUTES': os.environ.get('BACKUP_SCHEDULE_MINUTES', '120'),
        'IMMEDIATE_BACKUP_TEST': os.environ.get('IMMEDIATE_BACKUP_TEST', 'false'),
        'FORCE_BACKUP_ON_START': os.environ.get('FORCE_BACKUP_ON_START', 'false'),
        'DEBUG_GITHUB_BACKUP': os.environ.get('DEBUG_GITHUB_BACKUP', 'false')
    }
    
    logger.info("üìã Configuration compl√®te:")
    for var, value in config_vars.items():
        logger.info(f"   {var}: {value}")
    
    if not os.environ.get('GITHUB_TOKEN'):
        logger.error("üö® CONFIGURATION REQUISE:")
        logger.error("   1. Cr√©er Personal Access Token sur GitHub")
        logger.error("   2. Ajouter GITHUB_TOKEN sur Render")
        logger.error("   3. Red√©marrer le service")

# CORRECTION 6 : Auto-start am√©lior√©
if __name__ != "__main__":  # Quand import√© par app.py
    # Afficher les infos de configuration
    setup_github_backup_info()
    
    # CORRECTION MAJEURE : Thread NON daemon pour persistence
    backup_thread = threading.Thread(target=start_backup_scheduler, daemon=False)
    backup_thread.start()
    
    logger.info("üéØ GitHub Backup System CORRIG√â d√©marr√© !")

# NOUVEAU : Fonction de test direct
def test_backup_immediate():
    """Fonction de test pour backup imm√©diat"""
    logger.info("üß™ TEST BACKUP IMM√âDIAT")
    backup_manager = GitHubBackupManager()
    result = backup_manager.run_backup_cycle()
    logger.info(f"üèÅ R√©sultat test: {'‚úÖ SUCC√àS' if result else '‚ùå √âCHEC'}")
    return result

if __name__ == "__main__":
    # Test direct
    logger.info("üß™ Mode test GitHub backup")
    setup_github_backup_info()
    test_backup_immediate()
